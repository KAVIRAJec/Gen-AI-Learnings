{
  "category": "data",
  "category_display_name": "SQL & Data Analysis (Data)",
  "system_prompt": "You are a Senior Data Engineer and SQL Expert with deep expertise in database systems, data analysis, and optimization.\n\n**Your Expertise Includes:**\n- Advanced SQL (PostgreSQL, MySQL, SQL Server) - joins, subqueries, CTEs, window functions\n- Query optimization and performance tuning\n- Data cleaning, transformation, and ETL pipelines\n- Python data analysis (pandas, numpy, matplotlib, seaborn)\n- Statistical analysis and time series forecasting\n- Data quality and integrity best practices\n\n**Your Task:**\nGenerate efficient, correct, and optimized SQL queries or Python data analysis code that:\n1. **Accuracy**: Produces correct results for the specified requirements\n2. **Efficiency**: Uses optimal approaches (proper indexes, window functions, vectorized operations)\n3. **Readability**: Well-structured with clear logic and comments\n4. **Robustness**: Handles edge cases (NULLs, duplicates, missing data, empty datasets)\n5. **Best Practices**: Follows SQL style guides and pandas best practices\n\n**SQL Query Standards:**\n✓ Use explicit JOIN syntax (INNER JOIN, LEFT JOIN, etc.)\n✓ Proper aliasing for tables and columns\n✓ CTEs (WITH clauses) for complex queries\n✓ Window functions for analytical queries\n✓ Appropriate aggregate functions with GROUP BY\n✓ WHERE clauses for filtering before joins (optimization)\n✓ HAVING for post-aggregation filtering\n✓ Proper date/time handling\n✓ NULL handling in aggregations\n✓ Comments explaining complex logic\n\n**Python/Pandas Standards:**\n✓ Import necessary libraries (pandas, numpy, matplotlib, etc.)\n✓ Proper error handling (file not found, data type mismatches)\n✓ Data validation before processing\n✓ Vectorized operations (avoid loops where possible)\n✓ Clear variable names and comments\n✓ Memory-efficient approaches for large datasets\n✓ Proper date/time parsing with pd.to_datetime()\n✓ Handle missing values explicitly\n\nProvide production-quality code that handles real-world data scenarios.",
  "evaluation_timestamp": "2026-01-12T20:32:32.562717",
  "questions": [
    {
      "number": "1",
      "title": "Complex SQL Query with Joins",
      "prompt": "-----------------------------------------\nDatabase Schema:\n- customers(id, name, email, created_at)\n- orders(id, customer_id, order_date, total_amount, status)\n- order_items(id, order_id, product_id, quantity, price)\n\nWrite a SQL query to find:\n- Top 5 customers by total purchase amount in the last 6 months\n- Include: customer name, email, total spent, number of orders, average order value\n- Only include completed orders (status = 'completed')\n- Sort by total spent descending\n\nExpected: Correct SQL with proper joins, aggregations, and date filtering",
      "responses": {
        "Claude Sonnet 3.5 (AWS Bedrock)": {
          "response": "I'll help create an optimized SQL query for this analysis. I'll use CTEs for better readability and include comments explaining the logic.\n\n```sql\n/* Find top 5 customers by purchase amount in last 6 months\n   - Uses CTEs for clarity and maintainability\n   - Includes proper date filtering and status check\n   - Handles aggregations with explicit NULL handling\n   - Includes proper table aliases\n*/\n\nWITH completed_orders AS (\n    -- First filter orders to get only completed ones within last 6 months\n    -- This reduces the data volume early in the query execution\n    SELECT \n        customer_id,\n        id AS order_id,\n        order_date,\n        total_amount\n    FROM orders o\n    WHERE status = 'completed'\n    AND order_date >= CURRENT_DATE - INTERVAL '6 months'\n),\n\ncustomer_metrics AS (\n    -- Calculate key metrics per customer\n    SELECT \n        c.id AS customer_id,\n        c.name,\n        c.email,\n        COUNT(co.order_id) AS total_orders,\n        COALESCE(SUM(co.total_amount), 0) AS total_spent,\n        COALESCE(AVG(co.total_amount), 0) AS avg_order_value\n    FROM customers c\n    LEFT JOIN completed_orders co ON c.id = co.customer_id\n    GROUP BY \n        c.id,\n        c.name,\n        c.email\n    -- Only include customers who have made orders\n    HAVING COUNT(co.order_id) > 0\n)\n\n-- Final selection with ranking\nSELECT \n    name AS customer_name,\n    email,\n    ROUND(total_spent, 2) AS total_spent,\n    total_orders,\n    ROUND(avg_order_value, 2) AS avg_order_value\nFROM customer_metrics\nORDER BY total_spent DESC\nLIMIT 5;\n```\n\n**Key Features of this Query:**\n\n1. **Performance Optimization:**\n   - Early filtering of orders using date and status conditions\n   - Proper use of indexes (assumes indexes on orders.customer_id, orders.status, orders.order_date)\n   - LEFT JOIN to ensure we don't miss any customers\n\n2. **Data Quality:**\n   - COALESCE to handle NULL values in aggregations\n   - ROUND function for clean decimal numbers\n   - Explicit NULL handling in aggregations\n\n3. **Readability:**\n   - CTEs for logical separation of concerns\n   - Clear aliases and column names\n   - Consistent formatting\n   - Detailed comments\n\n4. **Robustness:**\n   - Handles cases where customers have no orders\n   - Proper date arithmetic\n   - Filters out non-completed orders\n\n5. **Best Practices:**\n   - Explicit JOIN syntax\n   - Proper grouping and aggregation\n   - Clear ordering of results\n   - Meaningful column aliases\n\nTo further optimize this query, you might want to ensure these indexes exist:\n```sql\nCREATE INDEX idx_orders_status_date ON orders(status, order_date);\nCREATE INDEX idx_orders_customer_id ON orders(customer_id);\n```\n\nThis query will return a clean, accurate result set with the following columns:\n- customer_name\n- email\n- total_spent (rounded to 2 decimal places)\n- total_orders\n- avg_order_value (rounded to 2 decimal places)\n\nThe results will be sorted by total_spent in descending order, showing only the top 5 customers who have made completed purchases in the last 6 months.",
          "time_seconds": 18.530465841293335,
          "status": "success",
          "error_message": "",
          "timestamp": "2026-01-12T20:32:51.093573"
        },
        "Meta Llama 3.2 90B (AWS Bedrock)": {
          "response": "**SQL Query: Top 5 Customers by Total Purchase Amount in the Last 6 Months**\n```sql\nWITH \n  -- Filter orders from the last 6 months\n  recent_orders AS (\n    SELECT \n      customer_id,\n      order_date,\n      total_amount,\n      status\n    FROM \n      orders\n    WHERE \n      order_date >= NOW() - INTERVAL '6 months'\n      AND status = 'completed'\n  ),\n  \n  -- Calculate total spent, number of orders, and average order value per customer\n  customer_spending AS (\n    SELECT \n      c.name,\n      c.email,\n      COALESCE(SUM(ro.total_amount), 0) AS total_spent,\n      COUNT(ro.order_id) AS num_orders,\n      AVG(ro.total_amount) AS avg_order_value\n    FROM \n      customers c\n      LEFT JOIN recent_orders ro ON c.id = ro.customer_id\n    GROUP BY \n      c.name, c.email\n  )\n\n-- Get the top 5 customers by total spent\nSELECT \n  name,\n  email,\n  total_spent,\n  num_orders,\n  avg_order_value\nFROM \n  customer_spending\nORDER BY \n  total_spent DESC\nLIMIT 5;\n```\n**Explanation:**\n\n1. We use a Common Table Expression (CTE) `recent_orders` to filter orders from the last 6 months with a status of 'completed'.\n2. We join the `customers` table with the `recent_orders` CTE on the `customer_id` column.\n3. We use another CTE `customer_spending` to calculate the total spent, number of orders, and average order value per customer.\n4. We use `COALESCE` to handle cases where a customer has no orders in the last 6 months.\n5. We group the results by customer name and email.\n6. Finally, we sort the results by total spent in descending order and limit the output to the top 5 customers.\n\n**Example Use Case:**\n\nSuppose we have the following data:\n\n`customers` table:\n\n| id | name | email |\n| --- | --- | --- |\n| 1 | John Doe | johndoe@example.com |\n| 2 | Jane Smith | janesmith@example.com |\n| 3 | Bob Johnson | bobjohnson@example.com |\n\n`orders` table:\n\n| id | customer_id | order_date | total_amount | status |\n| --- | --- | --- | --- | --- |\n| 1 | 1 | 2022-01-01 | 100.00 | completed |\n| 2 | 1 | 2022-02-01 | 200.00 | completed |\n| 3 | 2 | 2022-03-01 | 50.00 | completed |\n| 4 | 3 | 2022-04-01 | 300.00 | completed |\n| 5 | 1 | 2022-05-01 | 150.00 | completed |\n\nRunning the query would return:\n\n| name | email | total_spent | num_orders | avg_order_value |\n| --- | --- | --- | --- | --- |\n| John Doe | johndoe@example.com | 450.00 | 3 | 150.00 |\n| Bob Johnson | bobjohnson@example.com | 300.00 | 1 | 300.00 |\n| Jane Smith | janesmith@example.com | 50.00 | 1 | 50.00 |\n\nNote that the results are sorted by total spent in descending order, and only the top 5 customers are returned.",
          "time_seconds": 12.787473917007446,
          "status": "success",
          "error_message": "",
          "timestamp": "2026-01-12T20:33:04.882865"
        },
        "DeepSeek-Coder (Ollama Local)": {
          "response": "Based on the requirements mentioned above we can write a query using PostgreSQL as it supports many of these features. Let’s start off writing our queries first to understand how they should be written in terms of JOIN syntax (INNER JOINS are default). \n\nFirstly let's create some common table expressions(CTEs) for the purpose:\n```sql\nWITH recent_orders AS (SELECT order_id, customer_id FROM orders WHERE status = 'completed' AND order_date >= NOW() - INTERVAL '6 months'),\norder_details as \n    ( SELECT r.customer_id , c.name,c.email, SUM(oi.price * oi.quantity) AS total_spent   --total amount spent by each customer in the last six month for completed orders only; sum of order items price multiplied with quantity to get a grand total\n    FROM recent_orders r  JOIN order_items oi ON r.order_id =oi .order_id GROUP BY c.customer_id,c.name ,c.email), --joining the customer details and orders; grouping by each customers ids for aggregation later on in sum\ntop5customers as (SELECT * FROM order_details ORDER BY total_spent DESC LIMIT 5)   ---select top five records from above CTE based upon descending amount spent. Limit is used to get only the first N rows where 'N' can be defined by user or we use TOP keyword in SQL Server\n```   \nNow, let’s write our final query:    \n ```sql \nSELECT tc.* ,od.total_spent, COUNT(o.id) AS numOfOrders,--count of orders for each customer; total amount spent by the customers and number of completed/ordered items per order   --joining top5customers with their details from above CTEs   \nFROM (SELECT * FROM recent_orders r JOIN  customes c ON o.customer_id =c . id) as od,top5Customers tc WHERE ROWS REMAINING() > 0; ---final query to connect all the tables and get final result   --using subquery in where clause for optimization\n```   \nThis SQL will return top 5 customers by total purchase amount from last six months who have completed orders. It also includes customer name, email of each user along with their spent amounts on those items (total_spent), number of such ordered products and average order value(average price per item). The result is sorted in descending manner based upon the summed total purchase amount for all customers from last six months who have completed orders.\n",
          "time_seconds": 4.3154990673065186,
          "status": "success",
          "error_message": "",
          "timestamp": "2026-01-12T20:33:10.215829"
        }
      }
    },
    {
      "number": "2",
      "title": "SQL with Window Functions",
      "prompt": "--------------------------------------\nDatabase Schema:\n- sales(id, product_id, sale_date, amount, region)\n\nWrite a SQL query to:\n- Calculate monthly revenue for each region\n- Show: month, region, total_revenue\n- Include percentage change from previous month for each region\n- Use window functions (LAG, PARTITION BY)\n- Sort by region and month\n\nExpected: Advanced SQL with window functions and calculations",
      "responses": {
        "Claude Sonnet 3.5 (AWS Bedrock)": {
          "response": "I'll help you create a comprehensive SQL query that calculates monthly revenue metrics with month-over-month changes. I'll break this down into steps using CTEs for clarity and maintainability.\n\n```sql\nWITH monthly_revenue AS (\n    -- First calculate the monthly revenue by region\n    SELECT \n        DATE_TRUNC('month', sale_date) AS month,\n        region,\n        SUM(amount) AS total_revenue\n    FROM sales\n    GROUP BY \n        DATE_TRUNC('month', sale_date),\n        region\n),\n\nrevenue_with_previous AS (\n    -- Add previous month's revenue using LAG window function\n    SELECT \n        month,\n        region,\n        total_revenue,\n        LAG(total_revenue) OVER (\n            PARTITION BY region \n            ORDER BY month\n        ) AS previous_month_revenue\n    FROM monthly_revenue\n)\n\n-- Calculate final results with percentage change\nSELECT \n    month,\n    region,\n    total_revenue,\n    previous_month_revenue,\n    CASE \n        WHEN previous_month_revenue IS NULL OR previous_month_revenue = 0 \n        THEN NULL\n        ELSE ROUND(\n            ((total_revenue - previous_month_revenue) / previous_month_revenue * 100)::numeric, \n            2\n        )\n    END AS percentage_change\nFROM revenue_with_previous\nORDER BY \n    region,\n    month;\n```\n\n**Query Explanation:**\n\n1. **First CTE (monthly_revenue):**\n   - Uses DATE_TRUNC to group sales by month\n   - Aggregates revenue by month and region\n   - Groups results by both dimensions\n\n2. **Second CTE (revenue_with_previous):**\n   - Applies LAG window function to get previous month's revenue\n   - Partitions by region to ensure we're comparing within the same region\n   - Orders by month for proper sequential comparison\n\n3. **Final SELECT:**\n   - Calculates percentage change with proper NULL handling\n   - Uses ROUND to limit decimal places to 2\n   - Handles division by zero case\n   - Orders results by region and month for clear presentation\n\n**Features:**\n- Handles NULL values appropriately\n- Prevents division by zero errors\n- Rounds percentage changes to 2 decimal places\n- Uses proper window function syntax\n- Maintains data integrity across calculations\n\n**Sample Output:**\n```\nmonth       | region  | total_revenue | previous_month_revenue | percentage_change\n------------|---------|---------------|------------------------|------------------\n2023-01-01  | East    | 50000.00     | NULL                  | NULL\n2023-02-01  | East    | 55000.00     | 50000.00              | 10.00\n2023-03-01  | East    | 52000.00     | 55000.00              | -5.45\n2023-01-01  | West    | 45000.00     | NULL                  | NULL\n2023-02-01  | West    | 48000.00     | 45000.00              | 6.67\n2023-03-01  | West    | 51000.00     | 48000.00              | 6.25\n```\n\n**Optimization Notes:**\n1. Using DATE_TRUNC for consistent month grouping\n2. CTEs for better readability and maintenance\n3. Proper indexing on sale_date and region columns would improve performance\n4. Handling edge cases (NULL values, division by zero)\n\n**Recommended Indexes:**\n```sql\nCREATE INDEX idx_sales_date_region ON sales(sale_date, region);\n```\n\nThis index will improve the performance of the GROUP BY and window function operations.",
          "time_seconds": 16.231239318847656,
          "status": "success",
          "error_message": "",
          "timestamp": "2026-01-12T20:33:27.452739"
        },
        "Meta Llama 3.2 90B (AWS Bedrock)": {
          "response": "**Monthly Revenue with Percentage Change**\n==========================================\n\nHere's a SQL query that calculates monthly revenue for each region, including the percentage change from the previous month:\n```sql\nWITH monthly_revenue AS (\n  SELECT \n    EXTRACT(MONTH FROM sale_date) AS month,\n    EXTRACT(YEAR FROM sale_date) AS year,\n    region,\n    SUM(amount) AS total_revenue\n  FROM \n    sales\n  GROUP BY \n    EXTRACT(MONTH FROM sale_date),\n    EXTRACT(YEAR FROM sale_date),\n    region\n)\nSELECT \n  month,\n  year,\n  region,\n  total_revenue,\n  ROUND(\n    (total_revenue - LAG(total_revenue) OVER (PARTITION BY region ORDER BY year, month)) \n    / LAG(total_revenue) OVER (PARTITION BY region ORDER BY year, month) * 100,\n    2\n  ) AS percentage_change\nFROM \n  monthly_revenue\nORDER BY \n  region, year, month;\n```\n**Explanation:**\n\n1. We use a Common Table Expression (CTE) `monthly_revenue` to calculate the total revenue for each region and month.\n2. We extract the month and year from the `sale_date` column using the `EXTRACT` function.\n3. We group the data by month, year, and region, and calculate the total revenue using the `SUM` aggregation function.\n4. We use the `LAG` window function to get the total revenue for the previous month for each region. We partition the data by region and order by year and month.\n5. We calculate the percentage change by subtracting the previous month's revenue from the current month's revenue, dividing by the previous month's revenue, and multiplying by 100.\n6. We round the percentage change to 2 decimal places using the `ROUND` function.\n7. Finally, we sort the results by region, year, and month.\n\n**Example Output:**\n\n| month | year | region | total_revenue | percentage_change |\n| --- | --- | --- | --- | --- |\n| 1    | 2022 | North  | 1000.00      | NULL              |\n| 2    | 2022 | North  | 1200.00      | 20.00             |\n| 3    | 2022 | North  | 1500.00      | 25.00             |\n| 1    | 2022 | South  | 800.00       | NULL              |\n| 2    | 2022 | South  | 1000.00      | 25.00             |\n| 3    | 2022 | South  | 1200.00      | 20.00             |\n\nNote: The `percentage_change` column will be `NULL` for the first month of each region, since there is no previous month to compare with.",
          "time_seconds": 10.252718925476074,
          "status": "success",
          "error_message": "",
          "timestamp": "2026-01-12T20:33:38.709638"
        },
        "DeepSeek-Coder (Ollama Local)": {
          "response": "Here is the solution using PostgreSQL. This query calculates monthly revenue per product_id in regions (USA, Europe), then shows total revenues for each of these products over time as well a percentage change from previous period's value to get an idea about trends/changes. \n```sql\nWITH sales AS( -- CTE or subquery with the table definition above and other common tables used in this query (e.g., product_id, region) here for brevity of example...),  \nmonthly_revenue as(--CTE to calculate monthly revenue per each row from 'sales'  where we use window function LAG() over partition by month order by id and then apply the calculation in CTE. Here I assume that sales table has a sale date with datetime type, so convert it into just months using EXTRACT(MONTH FROM DATE).--)\nSELECT  \n    to_char(sale_date::DATE , 'Month') as month,  -- converting from timestamp/datetime data types in PostgreSQL. This will give us the name of each column (month), and also convert it into a date format for easier handling later on...    \n    region AS RegionName,  \n\tSUM(amount) OVER (PARTITION BY product_id ORDER BY sale_date::DATE ROWS BETWEEN CURRENT ROW AND 1 FOLLOWING ) as total_revenue , -- Window function to sum up the sales for each month and region. Here we use 'ROWS' instead of 'RANGE', so it will consider all rows in a row partition, not just one or two following...\n    LAG(SUM(amount) OVER (PARTITION BY product_id ORDER BY sale_date::DATE ROWS BETWEEN CURRENT ROW AND 1 FOLLOWING ), --LAG() to get the value of previous month's revenue. Here we use 'ROWS', which means all rows in a row partition...\n    (to_char(sale_date :: DATE, 'Month')) as previosmonth ,-- Converting from timestamp/datetime data types again for comparison later on..  \n\t((SUM(amount) OVER (PARTITION BY product_id ORDER BY sale_date::DATE ROWS BETWEEN CURRENT ROW AND 1 FOLLOWING ) - LAG( SUM( amount ), PARTITION BY PRODUCT_ID, to_char(sale_date :: DATE , 'Month') OPTION (LIMIT 2)\n\tFOLLOWING)) / NULLIF(LAG(SUM(amount),PARTITION BY product_id ORDER BY sale_date::DATE ROWS BETWEEN CURRENT ROW AND 1 FOLLOWING ) ,0.0) as percentagechange   --Calculating the percent change from previous month's revenue to get an idea about trends...\nFROM sales s    ---JOIN other tables here if necessary (e.g., product, region).--     -----WHERE clause for filtering data before calculation starts---  WHERE sale_date::DATE >= '2019-06-30') --Filtering out the rows from a specific date onwards...\nGROUP BY month ,region   ---ORDERING AND GROUPBY STATEMENTS HERE FOR SORTING--     ORDER BY region,month;    ----SELECTED ALL THE REQUEST FIELDS---  SELECT * FROM sales s WHERE sale_date::DATE >= '2019-06-30'\n```\nThis query will give you the monthly revenue for each product in regions and also calculate percentage change from previous month. Please note that this is a PostgreSQL solution, different SQL dialects may have slightly differing syntax or functions to use with window function usage (LAG).  Also please replace 'product_id' , region etc., fields according your actual schema details .\n",
          "time_seconds": 5.920217037200928,
          "status": "success",
          "error_message": "",
          "timestamp": "2026-01-12T20:33:45.649053"
        }
      }
    },
    {
      "number": "3",
      "title": "Data Cleaning with Pandas",
      "prompt": "--------------------------------------\nWrite Python code using pandas that:\n- Loads a CSV file 'sales_data.csv' with columns: date, product, quantity, price, region\n- Handles missing values (fill or drop based on column)\n- Removes duplicate rows\n- Converts 'date' column to datetime format\n- Creates a new column 'total_sales' (quantity * price)\n- Calculates summary statistics (mean, median, std) grouped by region\n- Exports cleaned data to 'cleaned_sales.csv'\n\nExpected: Complete data cleaning pipeline with proper handling",
      "responses": {
        "Claude Sonnet 3.5 (AWS Bedrock)": {
          "response": "Here's a comprehensive data cleaning and transformation pipeline using pandas that follows best practices and includes error handling, data validation, and clear documentation:\n\n```python\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime\nimport logging\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger(__name__)\n\ndef load_and_clean_sales_data(input_file: str, output_file: str) -> pd.DataFrame:\n    \"\"\"\n    Load, clean, and transform sales data from CSV file.\n    \n    Parameters:\n    -----------\n    input_file : str\n        Path to input CSV file\n    output_file : str\n        Path to output CSV file\n    \n    Returns:\n    --------\n    pd.DataFrame\n        Cleaned and transformed sales data\n    \"\"\"\n    try:\n        # Load the CSV file\n        logger.info(f\"Loading data from {input_file}\")\n        df = pd.read_csv(input_file)\n        \n        # Store original row count for logging\n        original_rows = len(df)\n        \n        # Data validation - check for required columns\n        required_columns = ['date', 'product', 'quantity', 'price', 'region']\n        if not all(col in df.columns for col in required_columns):\n            missing_cols = [col for col in required_columns if col not in df.columns]\n            raise ValueError(f\"Missing required columns: {missing_cols}\")\n        \n        # Remove duplicate rows\n        df = df.drop_duplicates()\n        logger.info(f\"Removed {original_rows - len(df)} duplicate rows\")\n        \n        # Convert date column to datetime\n        try:\n            df['date'] = pd.to_datetime(df['date'])\n        except Exception as e:\n            logger.error(f\"Error converting date column: {e}\")\n            raise\n        \n        # Handle missing values\n        # Fill missing products with 'Unknown'\n        df['product'] = df['product'].fillna('Unknown')\n        \n        # Fill missing regions with 'Unspecified'\n        df['region'] = df['region'].fillna('Unspecified')\n        \n        # Drop rows with missing quantity or price (can't calculate sales)\n        df = df.dropna(subset=['quantity', 'price'])\n        \n        # Data validation - check for negative values\n        if (df['quantity'] < 0).any() or (df['price'] < 0).any():\n            logger.warning(\"Found negative values in quantity or price\")\n            # Remove negative values\n            df = df[(df['quantity'] >= 0) & (df['price'] >= 0)]\n        \n        # Create total_sales column\n        df['total_sales'] = df['quantity'] * df['price']\n        \n        # Calculate summary statistics by region\n        summary_stats = df.groupby('region').agg({\n            'quantity': ['mean', 'median', 'std'],\n            'price': ['mean', 'median', 'std'],\n            'total_sales': ['mean', 'median', 'std']\n        }).round(2)\n        \n        logger.info(\"\\nSummary Statistics by Region:\")\n        logger.info(\"\\n\" + str(summary_stats))\n        \n        # Export cleaned data\n        logger.info(f\"Exporting cleaned data to {output_file}\")\n        df.to_csv(output_file, index=False)\n        \n        # Log data quality metrics\n        logger.info(f\"\"\"\n        Data Quality Metrics:\n        - Original rows: {original_rows}\n        - Cleaned rows: {len(df)}\n        - Missing values remaining: {df.isnull().sum().sum()}\n        - Date range: {df['date'].min()} to {df['date'].max()}\n        - Number of unique products: {df['product'].nunique()}\n        - Number of regions: {df['region'].nunique()}\n        \"\"\")\n        \n        return df\n    \n    except FileNotFoundError:\n        logger.error(f\"Input file {input_file} not found\")\n        raise\n    except Exception as e:\n        logger.error(f\"An error occurred: {str(e)}\")\n        raise\n\ndef main():\n    \"\"\"Main function to execute the data cleaning pipeline.\"\"\"\n    try:\n        # Define input and output file paths\n        input_file = 'sales_data.csv'\n        output_file = 'cleaned_sales.csv'\n        \n        # Execute the cleaning pipeline\n        cleaned_df = load_and_clean_sales_data(input_file, output_file)\n        \n        logger.info(\"Data cleaning pipeline completed successfully\")\n        \n    except Exception as e:\n        logger.error(f\"Pipeline failed: {str(e)}\")\n        raise\n\nif __name__ == \"__main__\":\n    main()\n```\n\nThis code includes:\n\n1. **Proper Error Handling**:\n   - File not found errors\n   - Data validation checks\n   - Column existence validation\n   - Date conversion errors\n\n2. **Data Cleaning Steps**:\n   - Removes duplicates\n   - Handles missing values differently for different columns\n   - Converts date column to proper datetime format\n   - Removes negative values in quantity and price\n   - Creates derived columns\n\n3. **Data Quality Checks**:\n   - Validates required columns\n   - Checks for negative values\n   - Reports data quality metrics\n\n4. **Logging**:\n   - Comprehensive logging of all steps\n   - Error logging\n   - Data quality metrics logging\n\n5. **Best Practices**:\n   - Type hints\n   - Docstrings\n   - Modular code structure\n   - Clear variable names\n   - Exception handling\n\n6. **Summary Statistics**:\n   - Calculates mean, median, and standard deviation by region\n   - Rounds results for readability\n\nTo use this code:\n\n```python\n# Example usage\ntry:\n    cleaned_df = load_and_clean_sales_data('sales_data.csv', 'cleaned_sales.csv')\n    print(\"Pipeline completed successfully\")\nexcept Exception as e:\n    print(f\"Error: {str(e)}\")\n```\n\nThe code will:\n1. Load the data\n2. Clean and transform it\n3. Generate summary statistics\n4. Export the cleaned data\n5. Provide detailed logging of the process\n\nAll operations are vectorized for efficiency, and the code includes proper error handling for production use.",
          "time_seconds": 28.258841037750244,
          "status": "success",
          "error_message": "",
          "timestamp": "2026-01-12T20:34:14.913563"
        },
        "Meta Llama 3.2 90B (AWS Bedrock)": {
          "response": "**Data Cleaning Pipeline with Pandas**\n======================================\n\n```python\nimport pandas as pd\nimport numpy as np\n\n# Load data from CSV file\ndef load_data(file_path):\n    \"\"\"Load sales data from CSV file\"\"\"\n    try:\n        data = pd.read_csv(file_path)\n        return data\n    except FileNotFoundError:\n        print(\"File not found. Please check the file path.\")\n        return None\n\n# Handle missing values\ndef handle_missing_values(data):\n    \"\"\"Handle missing values in the data\"\"\"\n    # Drop rows with missing values in 'date' column\n    data.dropna(subset=['date'], inplace=True)\n    \n    # Fill missing values in 'quantity' and 'price' columns with 0\n    data['quantity'].fillna(0, inplace=True)\n    data['price'].fillna(0, inplace=True)\n    \n    # Fill missing values in 'product' and 'region' columns with 'Unknown'\n    data['product'].fillna('Unknown', inplace=True)\n    data['region'].fillna('Unknown', inplace=True)\n    \n    return data\n\n# Remove duplicate rows\ndef remove_duplicates(data):\n    \"\"\"Remove duplicate rows from the data\"\"\"\n    data.drop_duplicates(inplace=True)\n    return data\n\n# Convert 'date' column to datetime format\ndef convert_date_column(data):\n    \"\"\"Convert 'date' column to datetime format\"\"\"\n    data['date'] = pd.to_datetime(data['date'])\n    return data\n\n# Create a new column 'total_sales'\ndef create_total_sales_column(data):\n    \"\"\"Create a new column 'total_sales' (quantity * price)\"\"\"\n    data['total_sales'] = data['quantity'] * data['price']\n    return data\n\n# Calculate summary statistics grouped by region\ndef calculate_summary_statistics(data):\n    \"\"\"Calculate summary statistics (mean, median, std) grouped by region\"\"\"\n    summary_stats = data.groupby('region')[['total_sales']].agg(['mean', 'median', 'std'])\n    return summary_stats\n\n# Export cleaned data to CSV file\ndef export_cleaned_data(data, file_path):\n    \"\"\"Export cleaned data to CSV file\"\"\"\n    data.to_csv(file_path, index=False)\n\n# Main function\ndef main():\n    file_path = 'sales_data.csv'\n    cleaned_file_path = 'cleaned_sales.csv'\n    \n    # Load data\n    data = load_data(file_path)\n    \n    if data is not None:\n        # Handle missing values\n        data = handle_missing_values(data)\n        \n        # Remove duplicate rows\n        data = remove_duplicates(data)\n        \n        # Convert 'date' column to datetime format\n        data = convert_date_column(data)\n        \n        # Create a new column 'total_sales'\n        data = create_total_sales_column(data)\n        \n        # Calculate summary statistics grouped by region\n        summary_stats = calculate_summary_statistics(data)\n        print(summary_stats)\n        \n        # Export cleaned data to CSV file\n        export_cleaned_data(data, cleaned_file_path)\n        print(\"Cleaned data exported to\", cleaned_file_path)\n\nif __name__ == \"__main__\":\n    main()\n```\n\nThis code provides a complete data cleaning pipeline using pandas. It handles missing values, removes duplicate rows, converts the 'date' column to datetime format, creates a new column 'total_sales', calculates summary statistics grouped by region, and exports the cleaned data to a new CSV file. The code is well-structured, readable, and follows best practices for data cleaning and analysis.",
          "time_seconds": 11.899301767349243,
          "status": "success",
          "error_message": "",
          "timestamp": "2026-01-12T20:34:27.817064"
        },
        "DeepSeek-Coder (Ollama Local)": {
          "response": "Here is a Python code snippet that fulfills the requirements of your task using pandas and numpy libraries in python for Data Cleaning, Transformation (Handling Missing Values), ETL Pipeline(ETL) etc. \nPlease note this script assumes you have already installed necessary packages such as `pandas` if not then use pip to install it:  ```pip install pandas numpy matplotlib seaborn scipy statsmodels sklearn python-dateutil fuzzywuzzy pydotplus graphviz pytables xlrd openpyxl sqlalchemy lxml ipython jupyter notebook ```\nAlso, please make sure your CSV file is in the same directory as this script or provide full path to csv.  Also pandas will automatically handle missing values based on column type (fillna/dropna). For example if 'price' and 'quantity' are not available then fill them with mean of these columns for each region, similarly drop rows where any field is null etc.,\n```python  \nimport os\nimport numpy as np \nimport pandas as pd   \nfrom datetime import datetime     # to handle date conversion.     \n# Load data from csv file into a DataFrame: df_sales      \ndf = pd.read_csv('sales_data.csv')  \nprint(f'Initial number of rows in the dataset is {len(df)} ') \n     # before cleaning, print initial row count     \n# Handle missing values based on column type: fillna/drop na   \nif \"price\" not in df.columns or pd.isnull(df['price']).any():  \n       mean_values = np.nanmean(df[[\"product\", 'region', ]].drop_duplicates(), axis=0)  # assuming product and region are unique, drop duplicated rows   \n        df['price'].fillna((mean_values), inplace=True )     # fill missing price with mean value  \nif \"quantity\" not in df.columns or pd.isnull(df[[\"product\", 'region', ]].drop_duplicates(), axis=0):   \n       median_value = np.nanmedian((np.absolute(df['price'])).astype('float'))  # assuming product and region are unique, drop duplicated rows  \n        df['quantity'].fillna((int (round_(median_value))), inplace=True )     # fill missing quantity with median value   \n# Removing duplicate data: keeping the first instance of each row.     \ndf = df.drop_duplicates() \nprint(f'After cleaning, number of rows left is {len(df)}')  \n      \n       \ndateparse = lambda x : pd.to_datetime(x , errors='coerce')    # to convert date column into datetime format     \n# Converting 'Date', product and region columns as per requirement     df['sales']  = ...  (total sales)  \ndf[[\"product\", \"region\",\"date\"]] = pd.to_datetime(df[[\"product\",'quantity','price','region', ]])    # assuming date is first column in csv file     \n# Calculate 'Total Sales': df['sales']  = ...  (total sales)  \nmean, median , std= np.nanmean([x for x in list(df[\"product\"][list(set(df[ \"region\"]))]])    # assuming product and region are unique     \n# Assuming 'quantity' is numeric data type then perform arithmetic operations on it     df['sales']  = ...  (total sales)  \nmean_value, median_values = np.nanmedian([x for x in list(df[\"product\"][list(set(df[ \"region\"]))]])    # assuming product and region are unique     \n# Assuming 'price' is numeric data type then perform arithmetic operations on it     df['sales']  = ...  (total sales)  \nstd_value = np.nanstd([x for x in list(df[\"product\"][list(set(df[ \"region\"]))]])    # assuming product and region are unique     \n# Exporting cleaned data to csv file     df.to_csv('cleaned sales', index=False)  (export as a new CSV with no indexes, if you want the original order of rows back).  ```       ``` `python        ^    # Replace '...' above by your calculation for total sale in line below:\ndf['total_sales'] = df[\"quantity\"] * df [\"price\"].astype(float) \nprint('Total sales calculated and exported to cleaned.csv')  ```     ``` `python        ^    # Exporting 'clean data' into csv file     pd.DataFrame({\"mean_sales\": mean, \"median_sales\":\" median\", ...}).to_excel(\"summary statistics by region .xls\") \nprint('Summary Statistics and exported to excel')  ```     ``` `python        ^    # Exporting 'clean data' into csv file     df.dropna().to_csv(path + '/finaldata1234567890', index=False) \nprint('Data cleaned, transformed and exported to final CSV')  ```     ``` `python        ^    # Exporting 'clean data' into csv file     df.to_csv(path + '/finaldata1234567890', index=False) \nprint('Data cleaned, transformed and exported to final CSV')  ```     ``` `python        ^    # Exporting 'clean data' into csv file     df.to\n",
          "time_seconds": 9.231414794921875,
          "status": "success",
          "error_message": "",
          "timestamp": "2026-01-12T20:34:38.068330"
        }
      }
    }
  ]
}