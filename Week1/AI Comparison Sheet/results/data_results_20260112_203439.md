# DATA - AI Model Comparison Results

**Generated:** January 12, 2026 at 20:34:39

---

## System Prompt Used

```
You are a Senior Data Engineer and SQL Expert with deep expertise in database systems, data analysis, and optimization.

**Your Expertise Includes:**
- Advanced SQL (PostgreSQL, MySQL, SQL Server) - joins, subqueries, CTEs, window functions
- Query optimization and performance tuning
- Data cleaning, transformation, and ETL pipelines
- Python data analysis (pandas, numpy, matplotlib, seaborn)
- Statistical analysis and time series forecasting
- Data quality and integrity best practices

**Your Task:**
Generate efficient, correct, and optimized SQL queries or Python data analysis code that:
1. **Accuracy**: Produces correct results for the specified requirements
2. **Efficiency**: Uses optimal approaches (proper indexes, window functions, vectorized operations)
3. **Readability**: Well-structured with clear logic and comments
4. **Robustness**: Handles edge cases (NULLs, duplicates, missing data, empty datasets)
5. **Best Practices**: Follows SQL style guides and pandas best practices

**SQL Query Standards:**
âœ“ Use explicit JOIN syntax (INNER JOIN, LEFT JOIN, etc.)
âœ“ Proper aliasing for tables and columns
âœ“ CTEs (WITH clauses) for complex queries
âœ“ Window functions for analytical queries
âœ“ Appropriate aggregate functions with GROUP BY
âœ“ WHERE clauses for filtering before joins (optimization)
âœ“ HAVING for post-aggregation filtering
âœ“ Proper date/time handling
âœ“ NULL handling in aggregations
âœ“ Comments explaining complex logic

**Python/Pandas Standards:**
âœ“ Import necessary libraries (pandas, numpy, matplotlib, etc.)
âœ“ Proper error handling (file not found, data type mismatches)
âœ“ Data validation before processing
âœ“ Vectorized operations (avoid loops where possible)
âœ“ Clear variable names and comments
âœ“ Memory-efficient approaches for large datasets
âœ“ Proper date/time parsing with pd.to_datetime()
âœ“ Handle missing values explicitly

Provide production-quality code that handles real-world data scenarios.
```

---

## Question 1: Complex SQL Query with Joins

### Prompt
```
-----------------------------------------
Database Schema:
- customers(id, name, email, created_at)
- orders(id, customer_id, order_date, total_amount, status)
- order_items(id, order_id, product_id, quantity, price)

Write a SQL query to find:
- Top 5 customers by total purchase amount in the last 6 months
- Include: customer name, email, total spent, number of orders, average order value
- Only include completed orders (status = 'completed')
- Sort by total spent descending

Expected: Correct SQL with proper joins, aggregations, and date filtering
```


### ðŸ“Š Performance Metrics (1-5 Scale, AI-Evaluated)

| Model | Code Quality | Accuracy | Ease of Use | Speed | Explanation | Edge Cases | **Avg** |
|-------|--------------|----------|-------------|-------|-------------|------------|----------|
| Claude Sonnet 3.5 (AWS Bedrock) | â­â­â­â­â­ (5) | â­â­â­â­â­ (5) | â­â­â­â­â­ (5) | â­â­â­â˜†â˜† (3) | â­â­â­â­â­ (5) | â­â­â­â­â­ (5) | **4.7** |
| Meta Llama 3.2 90B (AWS Bedrock) | â­â­â­â­â­ (5) | â­â­â­â­â­ (5) | â­â­â­â­â­ (5) | â­â­â­â˜†â˜† (3) | â­â­â­â­â­ (5) | â­â­â­â­â˜† (4) | **4.5** |
| DeepSeek-Coder (Ollama Local) | â­â­â˜†â˜†â˜† (2) | â­â­â˜†â˜†â˜† (2) | â­â­â˜†â˜†â˜† (2) | â­â­â­â­â­ (5) | â­â­â­â˜†â˜† (3) | â­â˜†â˜†â˜†â˜† (1) | **2.5** |

#### ðŸ¤– AI Judge Reasoning

**Claude Sonnet 3.5 (AWS Bedrock):** The SQL query demonstrates exceptional production quality with optimized CTEs, proper indexing suggestions, and comprehensive documentation. It handles all edge cases including NULL values, empty results, and date filtering correctly. The code is well-structured with clear comments, meaningful aliases, and includes performance optimization considerations that would work well in production.

**Meta Llama 3.2 90B (AWS Bedrock):** The SQL query is exceptionally well-structured using CTEs for readability and maintainability, with proper joins and efficient filtering. The explanation is comprehensive with clear examples and sample data. Edge case handling is good with COALESCE for null values, though could potentially add input validation for date ranges. The code is production-ready with excellent documentation and follows SQL best practices.

**DeepSeek-Coder (Ollama Local):** The query has several syntax errors (ROWS REMAINING, incorrect table names) and doesn't properly calculate average order value as required. While the explanation attempts to break down the approach using CTEs, the final query is incorrect and wouldn't execute. No error handling or null checks are included, and the query structure could be significantly simplified.



### Detailed Responses

#### âœ… Claude Sonnet 3.5 (AWS Bedrock)

**Time:** 18.53s | **Status:** success

```
I'll help create an optimized SQL query for this analysis. I'll use CTEs for better readability and include comments explaining the logic.

```sql
/* Find top 5 customers by purchase amount in last 6 months
   - Uses CTEs for clarity and maintainability
   - Includes proper date filtering and status check
   - Handles aggregations with explicit NULL handling
   - Includes proper table aliases
*/

WITH completed_orders AS (
    -- First filter orders to get only completed ones within last 6 months
    -- This reduces the data volume early in the query execution
    SELECT 
        customer_id,
        id AS order_id,
        order_date,
        total_amount
    FROM orders o
    WHERE status = 'completed'
    AND order_date >= CURRENT_DATE - INTERVAL '6 months'
),

customer_metrics AS (
    -- Calculate key metrics per customer
    SELECT 
        c.id AS customer_id,
        c.name,
        c.email,
        COUNT(co.order_id) AS total_orders,
        COALESCE(SUM(co.total_amount), 0) AS total_spent,
        COALESCE(AVG(co.total_amount), 0) AS avg_order_value
    FROM customers c
    LEFT JOIN completed_orders co ON c.id = co.customer_id
    GROUP BY 
        c.id,
        c.name,
        c.email
    -- Only include customers who have made orders
    HAVING COUNT(co.order_id) > 0
)

-- Final selection with ranking
SELECT 
    name AS customer_name,
    email,
    ROUND(total_spent, 2) AS total_spent,
    total_orders,
    ROUND(avg_order_value, 2) AS avg_order_value
FROM customer_metrics
ORDER BY total_spent DESC
LIMIT 5;
```

**Key Features of this Query:**

1. **Performance Optimization:**
   - Early filtering of orders using date and status conditions
   - Proper use of indexes (assumes indexes on orders.customer_id, orders.status, orders.order_date)
   - LEFT JOIN to ensure we don't miss any customers

2. **Data Quality:**
   - COALESCE to handle NULL values in aggregations
   - ROUND function for clean decimal numbers
   - Explicit NULL handling in aggregations...
```

*Response truncated. Full response in JSON file.*

#### âœ… Meta Llama 3.2 90B (AWS Bedrock)

**Time:** 12.79s | **Status:** success

```
**SQL Query: Top 5 Customers by Total Purchase Amount in the Last 6 Months**
```sql
WITH 
  -- Filter orders from the last 6 months
  recent_orders AS (
    SELECT 
      customer_id,
      order_date,
      total_amount,
      status
    FROM 
      orders
    WHERE 
      order_date >= NOW() - INTERVAL '6 months'
      AND status = 'completed'
  ),
  
  -- Calculate total spent, number of orders, and average order value per customer
  customer_spending AS (
    SELECT 
      c.name,
      c.email,
      COALESCE(SUM(ro.total_amount), 0) AS total_spent,
      COUNT(ro.order_id) AS num_orders,
      AVG(ro.total_amount) AS avg_order_value
    FROM 
      customers c
      LEFT JOIN recent_orders ro ON c.id = ro.customer_id
    GROUP BY 
      c.name, c.email
  )

-- Get the top 5 customers by total spent
SELECT 
  name,
  email,
  total_spent,
  num_orders,
  avg_order_value
FROM 
  customer_spending
ORDER BY 
  total_spent DESC
LIMIT 5;
```
**Explanation:**

1. We use a Common Table Expression (CTE) `recent_orders` to filter orders from the last 6 months with a status of 'completed'.
2. We join the `customers` table with the `recent_orders` CTE on the `customer_id` column.
3. We use another CTE `customer_spending` to calculate the total spent, number of orders, and average order value per customer.
4. We use `COALESCE` to handle cases where a customer has no orders in the last 6 months.
5. We group the results by customer name and email.
6. Finally, we sort the results by total spent in descending order and limit the output to the top 5 customers.

**Example Use Case:**

Suppose we have the following data:

`customers` table:

| id | name | email |
| --- | --- | --- |
| 1 | John Doe | johndoe@example.com |
| 2 | Jane Smith | janesmith@example.com |
| 3 | Bob Johnson | bobjohnson@example.com |

`orders` table:

| id | customer_id | order_date | total_amount | status |
| --- | --- | --- | --- | --- |
| 1 | 1 | 2022-01-01 | 100.00 | completed |
| 2 | 1 | 2022-02-01 | ...
```

*Response truncated. Full response in JSON file.*

#### âœ… DeepSeek-Coder (Ollama Local)

**Time:** 4.32s | **Status:** success

```
Based on the requirements mentioned above we can write a query using PostgreSQL as it supports many of these features. Letâ€™s start off writing our queries first to understand how they should be written in terms of JOIN syntax (INNER JOINS are default). 

Firstly let's create some common table expressions(CTEs) for the purpose:
```sql
WITH recent_orders AS (SELECT order_id, customer_id FROM orders WHERE status = 'completed' AND order_date >= NOW() - INTERVAL '6 months'),
order_details as 
    ( SELECT r.customer_id , c.name,c.email, SUM(oi.price * oi.quantity) AS total_spent   --total amount spent by each customer in the last six month for completed orders only; sum of order items price multiplied with quantity to get a grand total
    FROM recent_orders r  JOIN order_items oi ON r.order_id =oi .order_id GROUP BY c.customer_id,c.name ,c.email), --joining the customer details and orders; grouping by each customers ids for aggregation later on in sum
top5customers as (SELECT * FROM order_details ORDER BY total_spent DESC LIMIT 5)   ---select top five records from above CTE based upon descending amount spent. Limit is used to get only the first N rows where 'N' can be defined by user or we use TOP keyword in SQL Server
```   
Now, letâ€™s write our final query:    
 ```sql 
SELECT tc.* ,od.total_spent, COUNT(o.id) AS numOfOrders,--count of orders for each customer; total amount spent by the customers and number of completed/ordered items per order   --joining top5customers with their details from above CTEs   
FROM (SELECT * FROM recent_orders r JOIN  customes c ON o.customer_id =c . id) as od,top5Customers tc WHERE ROWS REMAINING() > 0; ---final query to connect all the tables and get final result   --using subquery in where clause for optimization
```   
This SQL will return top 5 customers by total purchase amount from last six months who have completed orders. It also includes customer name, email of each user along with their spent amounts on those items (total_spent...
```

*Response truncated. Full response in JSON file.*

---

## Question 2: SQL with Window Functions

### Prompt
```
--------------------------------------
Database Schema:
- sales(id, product_id, sale_date, amount, region)

Write a SQL query to:
- Calculate monthly revenue for each region
- Show: month, region, total_revenue
- Include percentage change from previous month for each region
- Use window functions (LAG, PARTITION BY)
- Sort by region and month

Expected: Advanced SQL with window functions and calculations
```


### ðŸ“Š Performance Metrics (1-5 Scale, AI-Evaluated)

| Model | Code Quality | Accuracy | Ease of Use | Speed | Explanation | Edge Cases | **Avg** |
|-------|--------------|----------|-------------|-------|-------------|------------|----------|
| Claude Sonnet 3.5 (AWS Bedrock) | â­â­â­â­â­ (5) | â­â­â­â­â­ (5) | â­â­â­â­â­ (5) | â­â­â­â˜†â˜† (3) | â­â­â­â­â­ (5) | â­â­â­â­â­ (5) | **4.7** |
| Meta Llama 3.2 90B (AWS Bedrock) | â­â­â­â­â­ (5) | â­â­â­â­â­ (5) | â­â­â­â­â­ (5) | â­â­â­â˜†â˜† (3) | â­â­â­â­â­ (5) | â­â­â­â­â˜† (4) | **4.5** |
| DeepSeek-Coder (Ollama Local) | â­â­â˜†â˜†â˜† (2) | â­â­â˜†â˜†â˜† (2) | â­â˜†â˜†â˜†â˜† (1) | â­â­â­â­â˜† (4) | â­â­â­â˜†â˜† (3) | â­â˜†â˜†â˜†â˜† (1) | **2.2** |

#### ðŸ¤– AI Judge Reasoning

**Claude Sonnet 3.5 (AWS Bedrock):** The solution demonstrates exceptional production-quality SQL with proper CTEs, window functions, and comprehensive edge case handling. The code is well-structured with clear explanations, includes performance optimization tips via indexing, and handles all edge cases including NULL values and division by zero. The documentation is thorough with sample output and detailed explanations of each component.

**Meta Llama 3.2 90B (AWS Bedrock):** The SQL query is exceptionally well-structured using CTEs, proper window functions, and clear formatting. The solution includes comprehensive explanations, example output, and handles core requirements perfectly. While it handles most edge cases through NULL handling in LAG functions, it could benefit from additional validation for negative amounts or invalid dates, though these are typically handled at the database constraint level.

**DeepSeek-Coder (Ollama Local):** The SQL query has significant syntax errors and formatting issues that would prevent execution. While it attempts to use the correct concepts (CTEs, window functions), the implementation is incorrect with malformed PARTITION BY clauses and invalid syntax for percentage calculations. The comments are helpful but the code structure is confusing and would require substantial debugging to make functional.



### Detailed Responses

#### âœ… Claude Sonnet 3.5 (AWS Bedrock)

**Time:** 16.23s | **Status:** success

```
I'll help you create a comprehensive SQL query that calculates monthly revenue metrics with month-over-month changes. I'll break this down into steps using CTEs for clarity and maintainability.

```sql
WITH monthly_revenue AS (
    -- First calculate the monthly revenue by region
    SELECT 
        DATE_TRUNC('month', sale_date) AS month,
        region,
        SUM(amount) AS total_revenue
    FROM sales
    GROUP BY 
        DATE_TRUNC('month', sale_date),
        region
),

revenue_with_previous AS (
    -- Add previous month's revenue using LAG window function
    SELECT 
        month,
        region,
        total_revenue,
        LAG(total_revenue) OVER (
            PARTITION BY region 
            ORDER BY month
        ) AS previous_month_revenue
    FROM monthly_revenue
)

-- Calculate final results with percentage change
SELECT 
    month,
    region,
    total_revenue,
    previous_month_revenue,
    CASE 
        WHEN previous_month_revenue IS NULL OR previous_month_revenue = 0 
        THEN NULL
        ELSE ROUND(
            ((total_revenue - previous_month_revenue) / previous_month_revenue * 100)::numeric, 
            2
        )
    END AS percentage_change
FROM revenue_with_previous
ORDER BY 
    region,
    month;
```

**Query Explanation:**

1. **First CTE (monthly_revenue):**
   - Uses DATE_TRUNC to group sales by month
   - Aggregates revenue by month and region
   - Groups results by both dimensions

2. **Second CTE (revenue_with_previous):**
   - Applies LAG window function to get previous month's revenue
   - Partitions by region to ensure we're comparing within the same region
   - Orders by month for proper sequential comparison

3. **Final SELECT:**
   - Calculates percentage change with proper NULL handling
   - Uses ROUND to limit decimal places to 2
   - Handles division by zero case
   - Orders results by region and month for clear presentation

**Features:**
- Handles NULL values appropriately
- Prevents division by zero errors
-...
```

*Response truncated. Full response in JSON file.*

#### âœ… Meta Llama 3.2 90B (AWS Bedrock)

**Time:** 10.25s | **Status:** success

```
**Monthly Revenue with Percentage Change**
==========================================

Here's a SQL query that calculates monthly revenue for each region, including the percentage change from the previous month:
```sql
WITH monthly_revenue AS (
  SELECT 
    EXTRACT(MONTH FROM sale_date) AS month,
    EXTRACT(YEAR FROM sale_date) AS year,
    region,
    SUM(amount) AS total_revenue
  FROM 
    sales
  GROUP BY 
    EXTRACT(MONTH FROM sale_date),
    EXTRACT(YEAR FROM sale_date),
    region
)
SELECT 
  month,
  year,
  region,
  total_revenue,
  ROUND(
    (total_revenue - LAG(total_revenue) OVER (PARTITION BY region ORDER BY year, month)) 
    / LAG(total_revenue) OVER (PARTITION BY region ORDER BY year, month) * 100,
    2
  ) AS percentage_change
FROM 
  monthly_revenue
ORDER BY 
  region, year, month;
```
**Explanation:**

1. We use a Common Table Expression (CTE) `monthly_revenue` to calculate the total revenue for each region and month.
2. We extract the month and year from the `sale_date` column using the `EXTRACT` function.
3. We group the data by month, year, and region, and calculate the total revenue using the `SUM` aggregation function.
4. We use the `LAG` window function to get the total revenue for the previous month for each region. We partition the data by region and order by year and month.
5. We calculate the percentage change by subtracting the previous month's revenue from the current month's revenue, dividing by the previous month's revenue, and multiplying by 100.
6. We round the percentage change to 2 decimal places using the `ROUND` function.
7. Finally, we sort the results by region, year, and month.

**Example Output:**

| month | year | region | total_revenue | percentage_change |
| --- | --- | --- | --- | --- |
| 1    | 2022 | North  | 1000.00      | NULL              |
| 2    | 2022 | North  | 1200.00      | 20.00             |
| 3    | 2022 | North  | 1500.00      | 25.00             |
| 1    | 2022 | South  | 800.00       | NULL       ...
```

*Response truncated. Full response in JSON file.*

#### âœ… DeepSeek-Coder (Ollama Local)

**Time:** 5.92s | **Status:** success

```
Here is the solution using PostgreSQL. This query calculates monthly revenue per product_id in regions (USA, Europe), then shows total revenues for each of these products over time as well a percentage change from previous period's value to get an idea about trends/changes. 
```sql
WITH sales AS( -- CTE or subquery with the table definition above and other common tables used in this query (e.g., product_id, region) here for brevity of example...),  
monthly_revenue as(--CTE to calculate monthly revenue per each row from 'sales'  where we use window function LAG() over partition by month order by id and then apply the calculation in CTE. Here I assume that sales table has a sale date with datetime type, so convert it into just months using EXTRACT(MONTH FROM DATE).--)
SELECT  
    to_char(sale_date::DATE , 'Month') as month,  -- converting from timestamp/datetime data types in PostgreSQL. This will give us the name of each column (month), and also convert it into a date format for easier handling later on...    
    region AS RegionName,  
	SUM(amount) OVER (PARTITION BY product_id ORDER BY sale_date::DATE ROWS BETWEEN CURRENT ROW AND 1 FOLLOWING ) as total_revenue , -- Window function to sum up the sales for each month and region. Here we use 'ROWS' instead of 'RANGE', so it will consider all rows in a row partition, not just one or two following...
    LAG(SUM(amount) OVER (PARTITION BY product_id ORDER BY sale_date::DATE ROWS BETWEEN CURRENT ROW AND 1 FOLLOWING ), --LAG() to get the value of previous month's revenue. Here we use 'ROWS', which means all rows in a row partition...
    (to_char(sale_date :: DATE, 'Month')) as previosmonth ,-- Converting from timestamp/datetime data types again for comparison later on..  
	((SUM(amount) OVER (PARTITION BY product_id ORDER BY sale_date::DATE ROWS BETWEEN CURRENT ROW AND 1 FOLLOWING ) - LAG( SUM( amount ), PARTITION BY PRODUCT_ID, to_char(sale_date :: DATE , 'Month') OPTION (LIMIT 2)
	FOLLOWING)) / NULLIF(LAG(SUM(amount...
```

*Response truncated. Full response in JSON file.*

---

## Question 3: Data Cleaning with Pandas

### Prompt
```
--------------------------------------
Write Python code using pandas that:
- Loads a CSV file 'sales_data.csv' with columns: date, product, quantity, price, region
- Handles missing values (fill or drop based on column)
- Removes duplicate rows
- Converts 'date' column to datetime format
- Creates a new column 'total_sales' (quantity * price)
- Calculates summary statistics (mean, median, std) grouped by region
- Exports cleaned data to 'cleaned_sales.csv'

Expected: Complete data cleaning pipeline with proper handling
```


### ðŸ“Š Performance Metrics (1-5 Scale, AI-Evaluated)

| Model | Code Quality | Accuracy | Ease of Use | Speed | Explanation | Edge Cases | **Avg** |
|-------|--------------|----------|-------------|-------|-------------|------------|----------|
| Claude Sonnet 3.5 (AWS Bedrock) | â­â­â­â­â­ (5) | â­â­â­â­â­ (5) | â­â­â­â­â­ (5) | â­â­â˜†â˜†â˜† (2) | â­â­â­â­â­ (5) | â­â­â­â­â­ (5) | **4.5** |
| Meta Llama 3.2 90B (AWS Bedrock) | â­â­â­â­â­ (5) | â­â­â­â­â­ (5) | â­â­â­â­â­ (5) | â­â­â­â˜†â˜† (3) | â­â­â­â­â­ (5) | â­â­â­â­â˜† (4) | **4.5** |
| DeepSeek-Coder (Ollama Local) | â­â­â˜†â˜†â˜† (2) | â­â­â˜†â˜†â˜† (2) | â­â˜†â˜†â˜†â˜† (1) | â­â­â­â­â˜† (4) | â­â­â­â˜†â˜† (3) | â­â­â˜†â˜†â˜† (2) | **2.3** |

#### ðŸ¤– AI Judge Reasoning

**Claude Sonnet 3.5 (AWS Bedrock):** The code demonstrates exceptional production-quality with comprehensive error handling, logging, input validation, and detailed documentation. It follows best practices with type hints, modular design, and clear function structure. The solution handles all edge cases including missing values, negative numbers, and file errors while providing detailed logging and data quality metrics.

**Meta Llama 3.2 90B (AWS Bedrock):** The code demonstrates excellent structure with modular functions, clear documentation, and proper error handling for file operations. It meets all requirements with a well-organized data cleaning pipeline and includes helpful docstrings for each function. While it handles basic error cases, it could be enhanced with more robust data validation and type checking for column values.

**DeepSeek-Coder (Ollama Local):** The code has significant issues including syntax errors, incomplete implementations, and confusing logic (especially in the date parsing and statistics calculation sections). While it attempts to handle some edge cases with missing values, the implementation is flawed. The explanatory comments are helpful but the code itself is not production-ready or immediately usable due to broken syntax and incomplete functionality.



### Detailed Responses

#### âœ… Claude Sonnet 3.5 (AWS Bedrock)

**Time:** 28.26s | **Status:** success

```
Here's a comprehensive data cleaning and transformation pipeline using pandas that follows best practices and includes error handling, data validation, and clear documentation:

```python
import pandas as pd
import numpy as np
from datetime import datetime
import logging

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def load_and_clean_sales_data(input_file: str, output_file: str) -> pd.DataFrame:
    """
    Load, clean, and transform sales data from CSV file.
    
    Parameters:
    -----------
    input_file : str
        Path to input CSV file
    output_file : str
        Path to output CSV file
    
    Returns:
    --------
    pd.DataFrame
        Cleaned and transformed sales data
    """
    try:
        # Load the CSV file
        logger.info(f"Loading data from {input_file}")
        df = pd.read_csv(input_file)
        
        # Store original row count for logging
        original_rows = len(df)
        
        # Data validation - check for required columns
        required_columns = ['date', 'product', 'quantity', 'price', 'region']
        if not all(col in df.columns for col in required_columns):
            missing_cols = [col for col in required_columns if col not in df.columns]
            raise ValueError(f"Missing required columns: {missing_cols}")
        
        # Remove duplicate rows
        df = df.drop_duplicates()
        logger.info(f"Removed {original_rows - len(df)} duplicate rows")
        
        # Convert date column to datetime
        try:
            df['date'] = pd.to_datetime(df['date'])
        except Exception as e:
            logger.error(f"Error converting date column: {e}")
            raise
        
        # Handle missing values
        # Fill missing products with 'Unknown'
        df['product'] = df['product'].fillna('Unknown')
        
        # Fill missing regions with 'Unspecified'
        df['region'] = df[...
```

*Response truncated. Full response in JSON file.*

#### âœ… Meta Llama 3.2 90B (AWS Bedrock)

**Time:** 11.90s | **Status:** success

```
**Data Cleaning Pipeline with Pandas**
======================================

```python
import pandas as pd
import numpy as np

# Load data from CSV file
def load_data(file_path):
    """Load sales data from CSV file"""
    try:
        data = pd.read_csv(file_path)
        return data
    except FileNotFoundError:
        print("File not found. Please check the file path.")
        return None

# Handle missing values
def handle_missing_values(data):
    """Handle missing values in the data"""
    # Drop rows with missing values in 'date' column
    data.dropna(subset=['date'], inplace=True)
    
    # Fill missing values in 'quantity' and 'price' columns with 0
    data['quantity'].fillna(0, inplace=True)
    data['price'].fillna(0, inplace=True)
    
    # Fill missing values in 'product' and 'region' columns with 'Unknown'
    data['product'].fillna('Unknown', inplace=True)
    data['region'].fillna('Unknown', inplace=True)
    
    return data

# Remove duplicate rows
def remove_duplicates(data):
    """Remove duplicate rows from the data"""
    data.drop_duplicates(inplace=True)
    return data

# Convert 'date' column to datetime format
def convert_date_column(data):
    """Convert 'date' column to datetime format"""
    data['date'] = pd.to_datetime(data['date'])
    return data

# Create a new column 'total_sales'
def create_total_sales_column(data):
    """Create a new column 'total_sales' (quantity * price)"""
    data['total_sales'] = data['quantity'] * data['price']
    return data

# Calculate summary statistics grouped by region
def calculate_summary_statistics(data):
    """Calculate summary statistics (mean, median, std) grouped by region"""
    summary_stats = data.groupby('region')[['total_sales']].agg(['mean', 'median', 'std'])
    return summary_stats

# Export cleaned data to CSV file
def export_cleaned_data(data, file_path):
    """Export cleaned data to CSV file"""
    data.to_csv(file_path, index=False)

# Main function
def main():
    file...
```

*Response truncated. Full response in JSON file.*

#### âœ… DeepSeek-Coder (Ollama Local)

**Time:** 9.23s | **Status:** success

```
Here is a Python code snippet that fulfills the requirements of your task using pandas and numpy libraries in python for Data Cleaning, Transformation (Handling Missing Values), ETL Pipeline(ETL) etc. 
Please note this script assumes you have already installed necessary packages such as `pandas` if not then use pip to install it:  ```pip install pandas numpy matplotlib seaborn scipy statsmodels sklearn python-dateutil fuzzywuzzy pydotplus graphviz pytables xlrd openpyxl sqlalchemy lxml ipython jupyter notebook ```
Also, please make sure your CSV file is in the same directory as this script or provide full path to csv.  Also pandas will automatically handle missing values based on column type (fillna/dropna). For example if 'price' and 'quantity' are not available then fill them with mean of these columns for each region, similarly drop rows where any field is null etc.,
```python  
import os
import numpy as np 
import pandas as pd   
from datetime import datetime     # to handle date conversion.     
# Load data from csv file into a DataFrame: df_sales      
df = pd.read_csv('sales_data.csv')  
print(f'Initial number of rows in the dataset is {len(df)} ') 
     # before cleaning, print initial row count     
# Handle missing values based on column type: fillna/drop na   
if "price" not in df.columns or pd.isnull(df['price']).any():  
       mean_values = np.nanmean(df[["product", 'region', ]].drop_duplicates(), axis=0)  # assuming product and region are unique, drop duplicated rows   
        df['price'].fillna((mean_values), inplace=True )     # fill missing price with mean value  
if "quantity" not in df.columns or pd.isnull(df[["product", 'region', ]].drop_duplicates(), axis=0):   
       median_value = np.nanmedian((np.absolute(df['price'])).astype('float'))  # assuming product and region are unique, drop duplicated rows  
        df['quantity'].fillna((int (round_(median_value))), inplace=True )     # fill missing quantity with median value   
# Removing duplic...
```

*Response truncated. Full response in JSON file.*

---
